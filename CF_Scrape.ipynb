{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "import json #for parsing the return from the Google API\n",
    "import urllib #for passing info to the Google API\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather the data, we will scrape the below website.  The site has an embedded table for each year including all the games played in that year.  They also include the Associated Press poll ranking for each team for every game.  1950 is the first year where the AP poll ranked teams in the pre-season, giving teams ranks for their first games.\n",
    "\n",
    "The goal here is to find the embedded table, extract it, save all the features as columns in a Pandas dataframe, combine all years, and save the final dataframe as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# test accessing the website\n",
    "year = 2017\n",
    "website = 'https://www.sports-reference.com/cfb/years/{}-schedule.html'.format(year)\n",
    "\n",
    "response = requests.get(website)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"no-js\" data-root=\"/home/cfb/build\" data-version=\"klecko-\" itemscope=\"\" itemtype=\"https://schema.org/WebSite\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"ie=edge\" http-equiv=\"x-ua-compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0, maximum-scale=2.0\" name=\"viewport\">\n",
      "   <link href=\"https://d2p3bygnnzw9w3.cloudfront.net/req/201910231\" rel=\"dns-prefetch\"/>\n",
      "   <!-- no:cookie fast load the css.           -->\n",
      "   <script>\n",
      "    function gup(n) {n = n.replace(/[\\[]/, '\\\\[').replace(/[\\]]/, '\\\\]'); var r = new RegExp('[\\\\?&]'+n+'=([^&#]*)'); var re = r.exec(location.search);   return re === null?'':decodeURIComponent(re[1].replace(/\\+/g,' '));}; document.srdev = gup('srdev')\n",
      "   </script>\n",
      "   <link crossorigin=\"\" href=\"https://d2p3bygnnzw9w3.cloudfront.net\" rel=\"preconnect\"/>\n",
      "   <link crossorigin=\"\" href=\"https://d17lgqwvsissft.cloudfront.net\" rel=\"preconnect\"/>\n",
      "   <style>\n",
      "    html,body{margin:0;padding:0;font:14px/1.25 \"Helvetica Neu\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85166\n",
      "\n",
      "874 Games Table\n",
      "\n",
      "\n",
      "\n",
      "Rk\n",
      "Wk\n",
      "Date\n",
      "Time\n",
      "Day\n",
      "Winner\n",
      "Pts\n",
      "\n",
      "Loser\n",
      "Pts\n",
      "Notes\n",
      "\n",
      "\n",
      "\n",
      "11Aug 26, 20173:00 PMSatBrigham Young20Portland State6LaVell Edwards Stadium - Provo, Utah\n",
      "21Aug 26, 20172:30 PMSatColorado State58Oregon State27Sonny Lubrick Field at Colorado State Stadium - Fort Collins, Colorado\n",
      "31Aug 26, 20176:00 PMSatHawaii38@Massachusetts35Warren McGuirk Alumni Stadium - Amherst, Massachusetts\n",
      "41Aug 26, 20177:30 PMSat(19)Â South Florida42@San Jose State22CEFCU Stadium - San Jose, California\n",
      "51Aug 26, 201\n"
     ]
    }
   ],
   "source": [
    "table = soup.find_all('table')\n",
    "print(len(table[0].text))\n",
    "print()\n",
    "print((table[0].text)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function in the larger function below to parse beautiful soup to lists.\n",
    "# This function has the added advantage of assigning None if the field does not exist, which\n",
    "# happens in earlier years of the data.\n",
    "def bs_to_list(bs):\n",
    "    text_list = []\n",
    "    if len (bs) > 0 :\n",
    "        for item in bs:\n",
    "            text_list.append(item.text)\n",
    "        return text_list\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950 starting now...\n",
      "1950 complete!\n",
      "1951 starting now...\n",
      "1951 complete!\n",
      "1952 starting now...\n",
      "1952 complete!\n",
      "1953 starting now...\n",
      "1953 complete!\n",
      "1954 starting now...\n",
      "1954 complete!\n",
      "1955 starting now...\n",
      "1955 complete!\n",
      "1956 starting now...\n",
      "1956 complete!\n",
      "1957 starting now...\n",
      "1957 complete!\n",
      "1958 starting now...\n",
      "1958 complete!\n",
      "1959 starting now...\n",
      "1959 complete!\n",
      "1960 starting now...\n",
      "1960 complete!\n",
      "1961 starting now...\n",
      "1961 complete!\n",
      "1962 starting now...\n",
      "1962 complete!\n",
      "1963 starting now...\n",
      "1963 complete!\n",
      "1964 starting now...\n",
      "1964 complete!\n",
      "1965 starting now...\n",
      "1965 complete!\n",
      "1966 starting now...\n",
      "1966 complete!\n",
      "1967 starting now...\n",
      "1967 complete!\n",
      "1968 starting now...\n",
      "1968 complete!\n",
      "1969 starting now...\n",
      "1969 complete!\n",
      "1970 starting now...\n",
      "1970 complete!\n",
      "1971 starting now...\n",
      "1971 complete!\n",
      "1972 starting now...\n",
      "1972 complete!\n",
      "1973 starting now...\n",
      "1973 complete!\n",
      "1974 starting now...\n",
      "1974 complete!\n",
      "1975 starting now...\n",
      "1975 complete!\n",
      "1976 starting now...\n",
      "1976 complete!\n",
      "1977 starting now...\n",
      "1977 complete!\n",
      "1978 starting now...\n",
      "1978 complete!\n",
      "1979 starting now...\n",
      "1979 complete!\n",
      "1980 starting now...\n",
      "1980 complete!\n",
      "1981 starting now...\n",
      "1981 complete!\n",
      "1982 starting now...\n",
      "1982 complete!\n",
      "1983 starting now...\n",
      "1983 complete!\n",
      "1984 starting now...\n",
      "1984 complete!\n",
      "1985 starting now...\n",
      "1985 complete!\n",
      "1986 starting now...\n",
      "1986 complete!\n",
      "1987 starting now...\n",
      "1987 complete!\n",
      "1988 starting now...\n",
      "1988 complete!\n",
      "1989 starting now...\n",
      "1989 complete!\n",
      "1990 starting now...\n",
      "1990 complete!\n",
      "1991 starting now...\n",
      "1991 complete!\n",
      "1992 starting now...\n",
      "1992 complete!\n",
      "1993 starting now...\n",
      "1993 complete!\n",
      "1994 starting now...\n",
      "1994 complete!\n",
      "1995 starting now...\n",
      "1995 complete!\n",
      "1996 starting now...\n",
      "1996 complete!\n",
      "1997 starting now...\n",
      "1997 complete!\n",
      "1998 starting now...\n",
      "1998 complete!\n",
      "1999 starting now...\n",
      "1999 complete!\n",
      "2000 starting now...\n",
      "2000 complete!\n",
      "2001 starting now...\n",
      "2001 complete!\n",
      "2002 starting now...\n",
      "2002 complete!\n",
      "2003 starting now...\n",
      "2003 complete!\n",
      "2004 starting now...\n",
      "2004 complete!\n",
      "2005 starting now...\n",
      "2005 complete!\n",
      "2006 starting now...\n",
      "2006 complete!\n",
      "2007 starting now...\n",
      "2007 complete!\n",
      "2008 starting now...\n",
      "2008 complete!\n",
      "2009 starting now...\n",
      "2009 complete!\n",
      "2010 starting now...\n",
      "2010 complete!\n",
      "2011 starting now...\n",
      "2011 complete!\n",
      "2012 starting now...\n",
      "2012 complete!\n",
      "2013 starting now...\n",
      "2013 complete!\n",
      "2014 starting now...\n",
      "2014 complete!\n",
      "2015 starting now...\n",
      "2015 complete!\n",
      "2016 starting now...\n",
      "2016 complete!\n",
      "2017 starting now...\n",
      "2017 complete!\n",
      "2018 starting now...\n",
      "2018 complete!\n"
     ]
    }
   ],
   "source": [
    "# make a dataframe to hold all the data.  create df outside of loop.\n",
    "df = pd.DataFrame()\n",
    "for year in range (1950,2019,1):\n",
    "    print(year, 'starting now...')\n",
    "\n",
    "    # set the website we scarpe from using the year variable\n",
    "    website = 'https://www.sports-reference.com/cfb/years/{}-schedule.html'.format(year)\n",
    "    # get the response, parse it with bs4, and find the table tags\n",
    "    response = requests.get(website)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find_all('table')\n",
    "    \n",
    "    # for all the desired data, find the associate data-stat tags\n",
    "    row =  table[0].find_all('th', {'scope':'row'})\n",
    "    week_number = table[0].find_all('td', {'data-stat':'week_number'})\n",
    "    date_game = table[0].find_all('td', {'data-stat':'date_game'})\n",
    "    time_game = table[0].find_all('td', {'data-stat':'time_game'})\n",
    "    day_game =  table[0].find_all('td', {'data-stat':'day_name'})\n",
    "    winner = table[0].find_all('td', {'data-stat':'winner_school_name'})\n",
    "    winner_pts  = table[0].find_all('td', {'data-stat':'winner_points'})\n",
    "    game_location = table[0].find_all('td', {'data-stat':'game_location'})\n",
    "    loser = table[0].find_all('td', {'data-stat':'loser_school_name'})\n",
    "    loser_pts = table[0].find_all('td', {'data-stat':'loser_points'})\n",
    "    notes = table[0].find_all('td', {'data-stat':'notes'})\n",
    "    \n",
    "    # make an empty temp df and add each row as a columns.\n",
    "    # this was neccessary to assign \"None\" to any fields that did not\n",
    "    # exist in earlier years\n",
    "    df_temp = pd.DataFrame()\n",
    "    \n",
    "    # iterate through the bs4\n",
    "    df_temp['row'] = bs_to_list(row)\n",
    "    df_temp['week_number'] = bs_to_list(week_number)\n",
    "    df_temp['winner'] = bs_to_list(winner)\n",
    "    df_temp['winner_pts'] = bs_to_list(winner_pts)\n",
    "    df_temp['loser'] = bs_to_list(loser)\n",
    "    df_temp['loser_pts'] = bs_to_list(loser_pts)\n",
    "    df_temp['game_date'] = bs_to_list(date_game)\n",
    "    df_temp['game_time'] = bs_to_list(time_game)\n",
    "    df_temp['game_day'] = bs_to_list(day_game)\n",
    "    df_temp['game_loc'] = bs_to_list(game_location)\n",
    "    df_temp['notes'] = bs_to_list(notes)\n",
    "    \n",
    "    # assign a new column for each df_temp with the year\n",
    "    df_temp['year'] = year\n",
    "    # append the newly generated df_temp to the full df\n",
    "    df = df.append(df_temp)\n",
    "    print(year, 'complete!')\n",
    "    # stop the scraper for a second to prevent overwhelming the website.\n",
    "    # the terms of use for the site requested scraping no faster than a human \n",
    "    # could access the data and I figured this was a good time interval.\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final file as a csv.  Append todays date to the filename\n",
    "# to keep for overwriting the full file and to document when the site\n",
    "# was scraped.\n",
    "today = str(date.today())\n",
    "df.to_csv('scraped_results/{}.csv'.format(today))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
